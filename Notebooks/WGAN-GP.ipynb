{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec9d32b-2637-4a18-8eb2-5291b2e1b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. PyTorch will use the GPU.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA is available. PyTorch will use the GPU.')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('CUDA is not available. PyTorch will use the CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140ba7b9-9379-4856-abe2-f666b411551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import grad\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47dab660-b1a0-4b0d-9998-f1c1ecac6186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Configuration\n",
    "# ================================\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 256\n",
    "Z_DIM = 128\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "EPOCHS = 300\n",
    "LEARNING_RATE = 1e-4\n",
    "SAMPLE_DIR = r\"D:\\\\Faisal\\\\WGAN\\\\generated_images\"\n",
    "METRICS_DIR = r\"D:\\\\Faisal\\\\WGAN\\\\metrics\"\n",
    "CHECKPOINT_PATH = \"D:\\\\Faisal\\\\WGAN\\\\checkpoint.pth\"\n",
    "start_epoch = 0\n",
    "\n",
    "os.makedirs(SAMPLE_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8cbc353-9166-4f15-8b7c-6a1fe612240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Dataset\n",
    "# ================================\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.images = [os.path.join(root_dir, img) for img in os.listdir(root_dir) if img.endswith('.png') or img.endswith('.jpg')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx]).convert('L')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "dataset = MRIDataset(r\"D:\\\\Faisal\\\\Datasets\\\\tumordatasetnew\\\\NO-PREPROCESSED\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8106e47f-6a6e-40af-b749-189d94716d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Models\n",
    "# ================================\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 4*4*1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (1024, 4, 4)),\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 4, 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(128, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(256, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(512, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.critic(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "354be732-5ae6-4fb4-bc70-2641cf3b8b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Gradient Penalty\n",
    "# ================================\n",
    "\n",
    "def gradient_penalty(critic, real, fake):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    epsilon = torch.rand((BATCH_SIZE, 1, 1, 1), device=DEVICE)\n",
    "    interpolated = real * epsilon + fake * (1 - epsilon)\n",
    "    interpolated.requires_grad_(True)\n",
    "\n",
    "    mixed_scores = critic(interpolated)\n",
    "    gradient = grad(outputs=mixed_scores, inputs=interpolated,\n",
    "                    grad_outputs=torch.ones_like(mixed_scores),\n",
    "                    create_graph=True, retain_graph=True)[0]\n",
    "    gradient = gradient.view(gradient.size(0), -1)\n",
    "    gp = ((gradient.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da38a00-0aae-438e-ae96-f562945cbc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Initialize models and optimizers\n",
    "# ================================\n",
    "\n",
    "gen = Generator(Z_DIM).to(DEVICE)\n",
    "critic = Critic().to(DEVICE)\n",
    "opt_gen = torch.optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))\n",
    "opt_critic = torch.optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(0.0, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdedcd-0227-46fd-b3b1-5b6aa8ae9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Faisal\\fypenv\\Lib\\site-packages\\torchmetrics\\utilities\\prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Inception and FID\n",
    "# ================================\n",
    "\n",
    "fid = FrechetInceptionDistance(normalize=True).to(DEVICE)\n",
    "inception = InceptionScore().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eabca28-97a3-4fd6-9b58-2f9a6b7ffe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Resumed training from epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\STUDENT ASUS # 10\\AppData\\Local\\Temp\\ipykernel_9016\\1583845462.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# Load Checkpoint\n",
    "# ================================\n",
    "\n",
    "gen_losses = []\n",
    "critic_losses = []\n",
    "\n",
    "if os.path.exists(CHECKPOINT_PATH):\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=DEVICE)\n",
    "    gen.load_state_dict(checkpoint[\"gen_state_dict\"])\n",
    "    critic.load_state_dict(checkpoint[\"critic_state_dict\"])\n",
    "    opt_gen.load_state_dict(checkpoint[\"opt_gen_state_dict\"])\n",
    "    opt_critic.load_state_dict(checkpoint[\"opt_critic_state_dict\"])\n",
    "    gen_losses = checkpoint[\"gen_losses\"]\n",
    "    critic_losses = checkpoint[\"critic_losses\"]\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    print(f\"âœ… Resumed training from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"ðŸ†• Starting training from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9994628e-0a31-444b-ba1b-fac211db280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ================================\n",
    "# Setup for FID and IS\n",
    "# ================================\n",
    "fid = FrechetInceptionDistance(feature=64).to(DEVICE)  # Keep FID calculation on the same device as your training\n",
    "inception = InceptionScore().to(DEVICE)  # Initialize InceptionScore\n",
    "\n",
    "# Create directories to save generated images and metrics\n",
    "os.makedirs(SAMPLE_DIR, exist_ok=True)\n",
    "os.makedirs(METRICS_DIR, exist_ok=True)\n",
    "\n",
    "# ================================\n",
    "# Training Loop\n",
    "# ================================\n",
    "fixed_noise = torch.randn(3, Z_DIM).to(DEVICE)\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    for real in tqdm(dataloader):\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.size(0)\n",
    "\n",
    "        # ================================\n",
    "        # Train Critic\n",
    "        # ================================\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(cur_batch_size, Z_DIM).to(DEVICE)\n",
    "            fake = gen(noise).detach()\n",
    "            critic_real = critic(real).reshape(-1)\n",
    "            critic_fake = critic(fake).reshape(-1)\n",
    "            gp = gradient_penalty(critic, real, fake)\n",
    "            loss_critic = -(critic_real.mean() - critic_fake.mean()) + LAMBDA_GP * gp\n",
    "\n",
    "            opt_critic.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            opt_critic.step()\n",
    "\n",
    "        # ================================\n",
    "        # Train Generator\n",
    "        # ================================\n",
    "        noise = torch.randn(cur_batch_size, Z_DIM).to(DEVICE)\n",
    "        fake = gen(noise)\n",
    "        output = critic(fake).reshape(-1)\n",
    "        loss_gen = -output.mean()\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "    gen_losses.append(loss_gen.item())\n",
    "    critic_losses.append(loss_critic.item())\n",
    "\n",
    "    # ================================\n",
    "    # Save Generated Images and Metrics\n",
    "    # ================================\n",
    "    with torch.no_grad():\n",
    "        fake_images = gen(fixed_noise).cpu()\n",
    "        for i in range(fake_images.size(0)):\n",
    "            save_image(fake_images[i], os.path.join(SAMPLE_DIR, f\"epoch_{epoch+1}_sample_{i+1}.png\"), normalize=True)\n",
    "\n",
    "        # Resize real and fake to 299x299\n",
    "        real_resized = F.interpolate(real, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "        fake_resized = F.interpolate(fake, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Convert 1-channel grayscale images to 3-channel by repeating\n",
    "        if real_resized.shape[1] == 1:\n",
    "            real_resized = real_resized.repeat(1, 3, 1, 1)\n",
    "        if fake_resized.shape[1] == 1:\n",
    "            fake_resized = fake_resized.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Convert to uint8 for FID and Inception Score\n",
    "        device = next(fid.parameters()).device\n",
    "        real_uint8 = ((real_resized + 1) * 127.5).clamp(0, 255).to(torch.uint8).to(device)\n",
    "        fake_uint8 = ((fake_resized + 1) * 127.5).clamp(0, 255).to(torch.uint8).to(device)\n",
    "\n",
    "        # Now it's safe to update\n",
    "        fid.update(real_uint8, real=True)\n",
    "        fid.update(fake_uint8, real=False)\n",
    "        inception.update(fake_uint8)\n",
    "\n",
    "        # Compute FID and IS scores\n",
    "        fid_score = fid.compute().item()\n",
    "        is_score = inception.compute()[0].item()\n",
    "\n",
    "        # Save the scores to a text file\n",
    "        with open(os.path.join(METRICS_DIR, \"scores.txt\"), 'a') as f:\n",
    "            f.write(f\"Epoch {epoch+1}: FID: {fid_score:.4f}, IS: {is_score:.4f}\\n\")\n",
    "\n",
    "    # Print the results for the current epoch\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}, FID: {fid_score:.4f}, IS: {is_score:.4f}\")\n",
    "\n",
    "    # Save the model checkpoints\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"gen_state_dict\": gen.state_dict(),\n",
    "        \"critic_state_dict\": critic.state_dict(),\n",
    "        \"opt_gen_state_dict\": opt_gen.state_dict(),\n",
    "        \"opt_critic_state_dict\": opt_critic.state_dict(),\n",
    "        \"gen_losses\": gen_losses,\n",
    "        \"critic_losses\": critic_losses,\n",
    "    }, CHECKPOINT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb814fa-6b97-43c8-b79d-9c6cea5b2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Save Loss Graph\n",
    "# ================================\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(gen_losses, label=\"Generator Loss\")\n",
    "plt.plot(critic_losses, label=\"Critic Loss\")\n",
    "plt.title('Training Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(METRICS_DIR, \"loss_plot.png\"))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fypenv - CUDA)",
   "language": "python",
   "name": "fypenv-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
